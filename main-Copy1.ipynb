{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "def read_user_id():\n",
    "    with open('./input.txt', 'r') as f:\n",
    "        return [l.strip().split(',') for l in  f.readlines()]\n",
    "\n",
    "\n",
    "def write_output(prediction):\n",
    "    with open('./output.txt', 'w') as f:\n",
    "        for pred in prediction:\n",
    "            f.write(pred+\"\\n\")\n",
    "\n",
    "\n",
    "def do(inputs):\n",
    "    string_results = []\n",
    "    for user, movie in inputs:\n",
    "        key_user = int(user)\n",
    "        key_movie = int(movie)\n",
    "#         print(\"(user, movie) = (%d, %d)\"%(key_user, key_movie))\n",
    "\n",
    "#         string_results.append(task2(key_user, key_movie))\n",
    "\n",
    "    return string_results\n",
    "\n",
    "\n",
    "def initialize_train_data():\n",
    "#     global movieIds, userIds\n",
    "    df_train = pd.read_csv('data/ratings_train.csv', usecols = ['userId', 'movieId', 'rating'])\n",
    "    return df_train\n",
    "#     movieIds = sorted(trainset['movieId'].unique())\n",
    "#     userIds = sorted(trainset['userId'].unique())\n",
    "    \n",
    "\n",
    "def initialize_test_data():\n",
    "    targets = pd.read_csv('data/ratings_test.csv', usecols = ['userId', 'movieId', 'rating'])\n",
    "    return targets\n",
    "\n",
    "\n",
    "def preprocess_by_user():\n",
    "    df_train.groupby(['userId']).rating.transform(lambda x : zscore(x,ddof=1)).head(3)\n",
    "\n",
    "\n",
    "def RMSE(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets)**2).mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global df_train\n",
    "    df_train = initialize_train_data()\n",
    "    df_valid = initialize_valid_data()\n",
    "    Mean = df_train.groupby(['userId']).rating.transform('mean')\n",
    "    Std = df_train.groupby(['userId']).rating.transform('std') \n",
    "    df_train['rating']=df_train.groupby(['userId']).rating.transform(lambda x : zscore(x, ddof=1))\n",
    "    movieIds = sorted(df_train['movieId'].unique())\n",
    "    userIds = sorted(df_train['userId'].unique())\n",
    "    df_ui = pd.DataFrame(index=sorted(movieIds), columns=sorted(userIds) )\n",
    "    for index, rows in  df_train.iterrows():\n",
    "         df_ui.loc[rows['movieId']][rows['userId']] = rows['rating']\n",
    "            \n",
    "        \n",
    "    \n",
    "#     df_svd = build_svd(df_train)\n",
    "#     # execute\n",
    "#     user_ids = read_user_id()\n",
    "#     result = do(user_ids)\n",
    "#     write_output(result)\n",
    "      \n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_svd(model, K=100):\n",
    "    filled_model= model.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    u, s, vh = np.linalg.svd(filled_model, True)\n",
    "    u = u[:,:K]\n",
    "    Sigma = np.diag(s[:K])\n",
    "    vh = vh[:K, :]\n",
    "    user_factors =  np.matmul(u, np.sqrt(Sigma))\n",
    "    item_factors =  np.matmul(np.sqrt(Sigma),vh)\n",
    "    df_prediction = pd.DataFrame(np.matmul(user_factors, item_factors),index=model.index, columns=model.columns)\n",
    "    df_prediction = df_prediction.round(4)\n",
    "    return df_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = build_svd(df_ui.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        0.0035\n",
       "2       -0.0085\n",
       "3        0.0434\n",
       "4       -0.0047\n",
       "5        0.0039\n",
       "          ...  \n",
       "83322   -0.0026\n",
       "83359   -0.0051\n",
       "83361   -0.0013\n",
       "83411   -0.0051\n",
       "83603   -0.0013\n",
       "Name: 1, Length: 6901, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict with svd \n",
    "# decomposition[int(uid)]\n",
    "decomposition[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6901, 515)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieIds = sorted(df_train['movieId'].unique())\n",
    "userIds = sorted(df_train['userId'].unique())\n",
    "len(movieIds), len(userIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleIterator(object):\n",
    "    \"\"\"\n",
    "    Randomly generate batches\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneEpochIterator(ShuffleIterator):\n",
    "    \"\"\"\n",
    "    Sequentially generate one-epoch batches, typically for test data\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "USER_NUM = 515\n",
    "ITEM_NUM = 6901\n",
    "DIM = 15\n",
    "EPOCH_MAX = 100\n",
    "DEVICE = \"/cpu:0\"\n",
    "\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)\n",
    "\n",
    "\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=val)])\n",
    "\n",
    "\n",
    "def build_deep(df_train):\n",
    "    samples_per_batch = df_train.shape[0] // BATCH_SIZE\n",
    "\n",
    "    iter_train = ShuffleIterator([df_train[\"userId\"],\n",
    "                                         df_train[\"movieId\"],\n",
    "                                         df_train[\"rating\"]],\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "\n",
    "    iter_valid = OneEpochIterator([df_valid[\"userId\"],\n",
    "                                         df_valid[\"movieId\"],\n",
    "                                         df_valid[\"rating\"]],\n",
    "                                        batch_size=-1)\n",
    "\n",
    "    use = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    item = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    infer, regularizer = ops.inference_svd(user_batch, item_batch, user_num=USER_NUM, item_num=ITEM_NUM, dim=DIM,device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = ops.optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.05, device=DEVICE)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"./log\", graph=sess.graph)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   item_batch: items,\n",
    "                                                                   rate_batch: rates})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items})\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print(\"{:3d} {:f} {:f} {:f}(s)\".format(i // samples_per_batch, train_err, test_err,\n",
    "                                                       end - start))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\", train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\", test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
